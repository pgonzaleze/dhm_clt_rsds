{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine learning algorithms for coral bleaching classification "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "'''\r\n",
    "    Import libraries\r\n",
    "'''\r\n",
    "from sklearn import datasets\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from numpy import mean\r\n",
    "from numpy import std\r\n",
    "import sklearn\r\n",
    "import seaborn as sb\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from numpy import mean \r\n",
    "from numpy import std\r\n",
    "import pingouin as pg   \r\n",
    "from scipy.stats import shapiro\r\n",
    "from scipy.stats import levene \r\n",
    "from scipy.stats import bartlett\r\n",
    "from scipy.stats import kruskal\r\n",
    "import scikit_posthocs as sp \r\n",
    "from scipy import stats\r\n",
    "from sklearn.datasets import make_classification\r\n",
    "from sklearn.metrics import confusion_matrix, classification_report\r\n",
    "from sklearn import metrics\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import RepeatedKFold\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.metrics import log_loss\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from itertools import combinations, permutations\r\n",
    "# check scikit-learn version\r\n",
    "print(sklearn.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.22.2.post1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\bmped\\Miniconda3\\lib\\site-packages\\outdated\\utils.py:18: OutdatedPackageWarning: The package pingouin is out of date. Your version is 0.3.10, the latest is 0.3.12.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  **kwargs\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "'''\r\n",
    "    Load full dataset\r\n",
    "'''\r\n",
    "data = pd.read_csv('df_sst_clouds_rsds_mon_t.csv',sep = \";\", low_memory=False)\r\n",
    "len(data) "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "33768"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "list(data.columns)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'ITEM_ID',\n",
       " 'COUNTRY',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'DHW_class',\n",
       " 'SEVERITY_CODE',\n",
       " 'MONTH',\n",
       " 'YEAR',\n",
       " 'CF_a_monmean',\n",
       " 'DHM',\n",
       " 'CLTa',\n",
       " 'severity_bin',\n",
       " 'sev_bin23',\n",
       " 'CLT2017',\n",
       " 'RSDS']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "'''\r\n",
    "    Subset DF by SEVERITY_CODE [0,1,2,3]\r\n",
    "      Severity_code == \"-1\" is dropped\r\n",
    "'''\r\n",
    "#data = data.dropna() # drop rows that contains NaN's \r\n",
    "data = data[(data.SEVERITY_CODE == 0)|(data.SEVERITY_CODE == 1)|(data.SEVERITY_CODE == 2)|(data.SEVERITY_CODE == 3)] \r\n",
    "#data = data[(data.YEAR >= 2002)] # First year with more than 100 records\r\n",
    "#data = data[(data.YEAR >= 2015) & (data.YEAR <= 2016)] # subset a single event\r\n",
    "#data = data[(data.YEAR >= 1997) & (data.YEAR <= 1998)]\r\n",
    "#list(data.columns)\r\n",
    "data = data.dropna() # drop rows that contains NaN's (if any)\r\n",
    "len(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "33602"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "        Create dataframes to build independent models \r\n",
    "        Example:\r\n",
    "        X1 = data[['var #1','var #2']]  ## features (dependen variable(s))\r\n",
    "        X2 = data[['var #1','var #3']] \r\n",
    "        y = data[['independent variable']]  ## labels (indipendent variable)\r\n",
    "        The object \"X_\" can be used to run a specific model, however, to keep it simple, the object \"X\" is going to be used and edited each time to get printed the results and avoid changing the code in subsequent steps\" \r\n",
    "'''\r\n",
    "# ## Features (dependent variable)\r\n",
    "# X0=data[['DHW']] # better than any othe DHW metric\r\n",
    "# X00=data[['DHW_9']] # better than DHW\r\n",
    "# X1=data[['DHW', 'CF']] \r\n",
    "# X2=data[['DHW', 'CFrunmean7']]\r\n",
    "# X3=data[['DHW', 'CFrunmean30']] \r\n",
    "# X4=data[['DHW', 'CFrunmean90']] # \r\n",
    "# X5=data[['DHW', 'CF_a']]\r\n",
    "# X6=data[['DHW', 'CF_a_runmean7']] #\r\n",
    "# X7=data[['DHW', 'CF_a_runmean30']] #\r\n",
    "# X8=data[['DHW', 'CF_a_runmean90']] # +\r\n",
    "# X9=data[['DHW_9','CF_a']] # + better than DHW_9\r\n",
    "# X10=data[['DHW_9','CFrunmean7']]\r\n",
    "# X11=data[['DHW_9','CFrunmean30']]\r\n",
    "# X12=data[['DHW_9','CFrunmean90']]\r\n",
    "# X13=data[['DHW_9','CF_a_runmean7']] # \r\n",
    "# X12=data[['DHW_9','CF_a_runmean7']] # +\r\n",
    "# X15=data[['DHW_9','CF_a_runmean90']] # \r\n",
    "# X16=data[['DHW_9','CFrunmean7','CD']] # x\r\n",
    "# X17=data[['DHW_9','CFrunmean7','WD']] # x\r\n",
    "# X18=data[['SSTrunmax90','CF_a_runmean90','WD']] # +++\r\n",
    "# X19=data[['DHW','CF_a_runmean7','CV_run7']] # +\r\n",
    "# X20=data[['DHW','CF_a_runmean30','CV_run30']] # +\r\n",
    "# X21=data[['DHW','CF_a_runmean90','CV_run90']] # -\r\n",
    "# X22=data[['SSTrunmean90','CF_a_runmean90','WD']] # +\r\n",
    "# X23=data[['SSTrunmean7','CF_a_runmean90','WD']]\r\n",
    "# X24=data[['SSTrunmax7','CF_a_runmean90','WD']] # ++\r\n",
    "# X25=data[['YEAR','DHW_9','CF_a_runmean7']] # +\r\n",
    "# X26=data[['YEAR','DHW','CF_a_runmean90']] # +\r\n",
    "# X27=data[['YEAR','CF_a_runmean90']] # -   VIF + 1,5\r\n",
    "# X28=data[['YEAR','SSTrunmax7','CFrunmean90']] # \r\n",
    "# X30=data[['YEAR','SSTrunmax90','DHW','CV_run90']] # +++\r\n",
    "# X31=data[['YEAR','SSTrunmax90','DHW','CV_run90','CFrunmean90']] # +++\r\n",
    "# X32=data[['DHW_adj_date', 'CFrunmean90_adj_date']] # YEAR + DHW_9 VIF + 1.5\r\n",
    "# #labels (indipendent variable)\r\n",
    "# y=data['SEVERITY_CODE'] "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "'''\r\n",
    "Iterable object to run the models \r\n",
    "Here you can set the model's desired variables\r\n",
    "'''\r\n",
    "X=data[['DHM', 'CLT2017', 'RSDS']]## Features (dependent variable(s)); select desired variables\r\n",
    "#X = data.loc[:,('DHW','CLT2017')]  ### ALTERNATIVE\r\n",
    "y=data['SEVERITY_CODE'] # labels (indipendent variable)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "'''\r\n",
    "    Variance inflation factor VIF\r\n",
    "'''\r\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\r\n",
    "# Get variables for which to compute VIF and add intercept term\r\n",
    "X['Intercept'] = 1\r\n",
    "# Compute and view VIF\r\n",
    "vif = pd.DataFrame()\r\n",
    "vif[\"variables\"] = X.columns\r\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\r\n",
    "# View results using print\r\n",
    "print(vif)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   variables       VIF\n",
      "0        DHM  1.000302\n",
      "1    CLT2017  2.276749\n",
      "2       RSDS  2.276309\n",
      "3  Intercept  1.592860\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\bmped\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random forest classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "'''\r\n",
    "    Build the models\r\n",
    "'''\r\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=3)\r\n",
    "model.fit(X,y)\r\n",
    "# evaluate the model\r\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=3)\r\n",
    "n_scores = cross_val_score(model, X, y, cv=cv) #n_jobs=-1, error_score='raise'"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "'''\r\n",
    "    Report performance\r\n",
    "'''\r\n",
    "print('Cross val score: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\r\n",
    "print('\\n')\r\n",
    "# Features importance\r\n",
    "print('=== features importances ===')\r\n",
    "fi = pd.DataFrame({'feature': list(X.columns),\r\n",
    "                   'importance': model.feature_importances_}).\\\r\n",
    "                    sort_values('importance', ascending = False)\r\n",
    "fi"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross val score: 0.655 (0.005)\n",
      "\n",
      "\n",
      "=== features importances ===\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     feature  importance\n",
       "1       RSDS    0.658001\n",
       "0        DHM    0.341999\n",
       "2  Intercept    0.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RSDS</td>\n",
       "      <td>0.658001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DHM</td>\n",
       "      <td>0.341999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "'''\r\n",
    "    Confusion matrix\r\n",
    "'''\r\n",
    "y_pred=model.predict(X)\r\n",
    "conf_mat = confusion_matrix(y, y_pred)\r\n",
    "conf_mat_norm = confusion_matrix(y, y_pred,normalize='all')\r\n",
    "print(\"=== Confusion matrix ===\")\r\n",
    "print(conf_mat)\r\n",
    "print('\\n')\r\n",
    "print(\"=== Confusion matrix normalized ===\")\r\n",
    "print(conf_mat_norm)\r\n",
    "print('\\n')\r\n",
    "print(\"=== Classification Report ===\")\r\n",
    "print(classification_report(y, y_pred))\r\n",
    "print('\\n')\r\n",
    "print('=== Accuracy and Kappa ===')\r\n",
    "print('accuracy', metrics.accuracy_score(y, y_pred))\r\n",
    "print('\\n')\r\n",
    "print('kappa', metrics.cohen_kappa_score(y, y_pred))\r\n",
    "print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Confusion matrix ===\n",
      "[[12998   840   350    52]\n",
      " [ 1610  4787   852    89]\n",
      " [  666   649  5599   543]\n",
      " [  144   100   892  3431]]\n",
      "\n",
      "\n",
      "=== Confusion matrix normalized ===\n",
      "[[0.38682221 0.02499851 0.01041605 0.00154753]\n",
      " [0.04791381 0.14246176 0.02535563 0.00264865]\n",
      " [0.01982025 0.01931433 0.16662699 0.01615975]\n",
      " [0.00428546 0.00297601 0.02654604 0.10210702]]\n",
      "\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.88     14240\n",
      "           1       0.75      0.65      0.70      7338\n",
      "           2       0.73      0.75      0.74      7457\n",
      "           3       0.83      0.75      0.79      4567\n",
      "\n",
      "    accuracy                           0.80     33602\n",
      "   macro avg       0.79      0.77      0.78     33602\n",
      "weighted avg       0.80      0.80      0.80     33602\n",
      "\n",
      "\n",
      "\n",
      "=== Accuracy and Kappa ===\n",
      "accuracy 0.7980179751205285\n",
      "\n",
      "\n",
      "kappa 0.7100709413276404\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "'''\r\n",
    "    Evaluation between classifications models through \"log loss\"\r\n",
    "'''\r\n",
    "model_probs = model.predict_proba(X)\r\n",
    "score = log_loss(y, model_probs)\r\n",
    "\r\n",
    "'''\r\n",
    "    Evaluation between classifications models through \"ROC_AUC\"\r\n",
    "'''\r\n",
    "roc_value = roc_auc_score(y, model_probs, multi_class='ovo') # ovo': Computes the average AUC of all possible pairwise combinations of classes\r\n",
    "\r\n",
    "\r\n",
    "print('=== roc_auc_score ===') \r\n",
    "print(roc_value)\r\n",
    "print(' ')\r\n",
    "print('=== log_loss_score ===') \r\n",
    "print(score)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== roc_auc_score ===\n",
      "0.954297793198757\n",
      " \n",
      "=== log_loss_score ===\n",
      "0.45824231578571356\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt     \r\n",
    "ax = plt.subplot()\r\n",
    "#Heat map with annot=True to annotate cells\r\n",
    "sns.heatmap(conf_mat, annot=True, ax = ax, fmt='d', cmap='Blues') # actual cases\r\n",
    "#sns.heatmap(conf_mat/np.sum(conf_mat), annot=True, ax = ax, fmt='.2%', cmap='Blues') #percentage\r\n",
    "# labels, title and ticks\r\n",
    "ax.set_xlabel('Predicted bleaching level');ax.set_ylabel('True bleaching level'); \r\n",
    "ax.set_title('Confusion Matrix'); \r\n",
    "ax.xaxis.set_ticklabels(['level 0','level 1','level 2', 'level 3']); ax.yaxis.set_ticklabels(['level 0','level 1','level 2', 'level 3'])\r\n",
    "#plt.show()\r\n",
    "#plt.savefig('DHW9_CF_a_runmean90.pdf', dpi=300)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "comparisons = pd.DataFrame({'Real':y, 'Predictions':y_pred})\r\n",
    "comparisons.to_csv('pred_DHW_CF_a_runeman90.csv')\r\n",
    "print(comparisons[['Real','Predictions']])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.to_csv('df_for_RFoutputs.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "interpreter": {
   "hash": "3bed7ae5c37a2f0124a3d1d7ebe9444ba8b4d4f68ec45ffe4ebc67430627124e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}