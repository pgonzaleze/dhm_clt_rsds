{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning algorithms for coral bleaching classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Import libraries\n",
    "'''\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import sklearn\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean \n",
    "from numpy import std\n",
    "import pingouin as pg   \n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import levene \n",
    "from scipy.stats import bartlett\n",
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp \n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from itertools import combinations, permutations\n",
    "# check scikit-learn version\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load full dataset\n",
    "'''\n",
    "data = pd.read_csv('df_sst_clouds_rsds_mon_t.csv',sep = \";\", low_memory=False)\n",
    "len(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Subset DF by SEVERITY_CODE [0,1,2,3]\n",
    "      Severity_code == \"-1\" is dropped\n",
    "'''\n",
    "#data = data.dropna() # drop rows that contains NaN's \n",
    "data = data[(data.SEVERITY_CODE == 0)|(data.SEVERITY_CODE == 1)|(data.SEVERITY_CODE == 2)|(data.SEVERITY_CODE == 3)] \n",
    "#data = data[(data.YEAR >= 2002)] # First year with more than 100 records\n",
    "#data = data[(data.YEAR >= 2015) & (data.YEAR <= 2016)] # subset a single event\n",
    "#data = data[(data.YEAR >= 1997) & (data.YEAR <= 1998)]\n",
    "#list(data.columns)\n",
    "data = data.dropna() # drop rows that contains NaN's (if any)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Iterable object to run the models \n",
    "Here you can set the model's desired variables\n",
    "'''\n",
    "X=data[['DHM', 'CLTa', 'RSDSa']]## Features (dependent variable(s)); select desired variables\n",
    "#X = data.loc[:,('DHW','CLTa', 'RSDSa')]  ### ALTERNATIVE\n",
    "y=data['SEVERITY_CODE'] # labels (indipendent variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Variance inflation factor VIF\n",
    "'''\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Get variables for which to compute VIF and add intercept term\n",
    "X['Intercept'] = 1\n",
    "# Compute and view VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variables\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "# View results using print\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Build the models\n",
    "'''\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=3)\n",
    "model.fit(X,y)\n",
    "# evaluate the model\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=3)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv) #n_jobs=-1, error_score='raise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Report performance\n",
    "'''\n",
    "print('Cross val score: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "print('\\n')\n",
    "# Features importance\n",
    "print('=== features importances ===')\n",
    "fi = pd.DataFrame({'feature': list(X.columns),\n",
    "                   'importance': model.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Confusion matrix\n",
    "'''\n",
    "y_pred=model.predict(X)\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "conf_mat_norm = confusion_matrix(y, y_pred,normalize='all')\n",
    "print(\"=== Confusion matrix ===\")\n",
    "print(conf_mat)\n",
    "print('\\n')\n",
    "print(\"=== Confusion matrix normalized ===\")\n",
    "print(conf_mat_norm)\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y, y_pred))\n",
    "print('\\n')\n",
    "print('=== Accuracy and Kappa ===')\n",
    "print('accuracy', metrics.accuracy_score(y, y_pred))\n",
    "print('\\n')\n",
    "print('kappa', metrics.cohen_kappa_score(y, y_pred))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Evaluation between classifications models through \"log loss\"\n",
    "'''\n",
    "model_probs = model.predict_proba(X)\n",
    "score = log_loss(y, model_probs)\n",
    "\n",
    "'''\n",
    "    Evaluation between classifications models through \"ROC_AUC\"\n",
    "'''\n",
    "roc_value = roc_auc_score(y, model_probs, multi_class='ovo') # ovo': Computes the average AUC of all possible pairwise combinations of classes\n",
    "\n",
    "\n",
    "print('=== roc_auc_score ===') \n",
    "print(roc_value)\n",
    "print(' ')\n",
    "print('=== log_loss_score ===') \n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "ax = plt.subplot()\n",
    "#Heat map with annot=True to annotate cells\n",
    "sns.heatmap(conf_mat, annot=True, ax = ax, fmt='d', cmap='Blues') # actual cases\n",
    "#sns.heatmap(conf_mat/np.sum(conf_mat), annot=True, ax = ax, fmt='.2%', cmap='Blues') #percentage\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted bleaching level');ax.set_ylabel('True bleaching level'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['level 0','level 1','level 2', 'level 3']); ax.yaxis.set_ticklabels(['level 0','level 1','level 2', 'level 3'])\n",
    "#plt.show()\n",
    "#plt.savefig('DHW9_CF_a_runmean90.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = pd.DataFrame({'Real':y, 'Predictions':y_pred})\n",
    "comparisons.to_csv('pred_DHW_CF_a_runeman90.csv')\n",
    "print(comparisons[['Real','Predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('df_for_RFoutputs.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bed7ae5c37a2f0124a3d1d7ebe9444ba8b4d4f68ec45ffe4ebc67430627124e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
